{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 248218 entries, 0 to 248217\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   name    248218 non-null  object\n",
      " 1   use0    248218 non-null  object\n",
      " 2   use1    73365 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 5.7+ MB\n",
      "None\n",
      "\n",
      "First few rows of the dataset:\n",
      "                       name  \\\n",
      "0  augmentin 625 duo tablet   \n",
      "1       azithral 500 tablet   \n",
      "2          ascoril ls syrup   \n",
      "3      allegra 120mg tablet   \n",
      "4            avil 25 tablet   \n",
      "\n",
      "                                                use0  \\\n",
      "0                  Treatment of Bacterial infections   \n",
      "1                  Treatment of Bacterial infections   \n",
      "2                      Treatment of Cough with mucus   \n",
      "3  Treatment of Sneezing and runny nose due to al...   \n",
      "4                   Treatment of Allergic conditions   \n",
      "\n",
      "                               use1  \n",
      "0                               NaN  \n",
      "1                               NaN  \n",
      "2                               NaN  \n",
      "3  Treatment of Allergic conditions  \n",
      "4                               NaN  \n",
      "Dataset cleaned and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import warnings\n",
    "import speech_recognition as sr  # Using speech_recognition instead of whisper\n",
    "\n",
    "# Suppress specific FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Function to preprocess text using spaCy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "# Function to preprocess and vectorize user input\n",
    "def preprocess_user_input(user_input, vectorizer):\n",
    "    combined_text = ' '.join([preprocess_text(value) for value in user_input.values()])\n",
    "    processed_text = vectorizer.transform([combined_text])\n",
    "    return processed_text\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'medicine_dataset.csv'\n",
    "data = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Keep only the specified columns\n",
    "columns_to_keep = ['name', 'use0', 'use1']\n",
    "data = data[columns_to_keep]\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(data.info())\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Handle missing values\n",
    "data.fillna('', inplace=True)\n",
    "\n",
    "# Clean text data\n",
    "def clean_text(text):\n",
    "    return str(text).lower().replace('_', ' ')\n",
    "\n",
    "for col in columns_to_keep:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].apply(clean_text)\n",
    "\n",
    "# Remove duplicates\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Save cleaned dataset\n",
    "cleaned_file_path = 'cleaned_medicine_dataset.csv'\n",
    "data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(\"Dataset cleaned and saved successfully.\")\n",
    "\n",
    "# Load cleaned data\n",
    "data = pd.read_csv(cleaned_file_path, low_memory=False)\n",
    "\n",
    "# Combine relevant columns into a single 'text' column for processing\n",
    "def combine_text(row):\n",
    "    return ' '.join([str(row['use0']), str(row['use1'])])\n",
    "\n",
    "data['combined_text'] = data.apply(combine_text, axis=1)\n",
    "\n",
    "# Preprocess the combined text in parallel\n",
    "def parallelize_dataframe(df, func):\n",
    "    num_cores = mp.cpu_count()\n",
    "    df_split = np.array_split(df, num_cores)\n",
    "    pool = mp.Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def preprocess_texts(df):\n",
    "    df['combined_text'] = df['combined_text'].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "data = parallelize_dataframe(data, preprocess_texts)\n",
    "\n",
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(data['combined_text'])\n",
    "y = data['name']\n",
    "\n",
    "# Save the processed data\n",
    "X_file_path = 'X_tfidf_vectors.npz'\n",
    "y_file_path = 'y_labels.csv'\n",
    "sp.save_npz(X_file_path, X)\n",
    "y.to_csv(y_file_path, index=False)\n",
    "\n",
    "print(\"Text data preprocessed and saved successfully.\")\n",
    "\n",
    "# Load processed data\n",
    "X = sp.load_npz(X_file_path)\n",
    "y = pd.read_csv(y_file_path)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y.values.ravel())\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and compile model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model with reduced sample size for quick testing\n",
    "sample_size = 5000  # Adjust this for quicker runs\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=sample_size, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train_sample, y_train_sample, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "# Use unique classes in y_test for the classification report\n",
    "unique_test_labels = np.unique(y_test)\n",
    "target_names = label_encoder.inverse_transform(unique_test_labels)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes, labels=unique_test_labels, target_names=target_names, zero_division=0))\n",
    "\n",
    "# Function to convert speech to text using SpeechRecognition\n",
    "def speech_to_text(audio_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = recognizer.record(source)\n",
    "    try:\n",
    "        return recognizer.recognize_google(audio)\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Speech Recognition could not understand audio\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Could not request results from Google Speech Recognition service; {e}\"\n",
    "\n",
    "# Get user input via text\n",
    "user_input = {\n",
    "    'primary_reason': input(\"What is your primary reason for seeking medication? \"),\n",
    "    'allergies': input(\"Do you have any known allergies or sensitivities to medications? \"),\n",
    "    'current_medications': input(\"Are you currently taking any other medications (prescription, over-the-counter, supplements)? \"),\n",
    "    'chronic_conditions': input(\"Do you have any chronic medical conditions (e.g., diabetes, hypertension, asthma)? \"),\n",
    "    'symptoms': input(\"Can you describe your symptoms in detail? When did they start? \")\n",
    "}\n",
    "\n",
    "# For audio input (uncomment and specify audio file path)\n",
    "# audio_path = 'path_to_audio_file.wav'\n",
    "# speech_text = speech_to_text(audio_path)\n",
    "# user_input['symptoms'] = speech_text\n",
    "\n",
    "# Preprocess and vectorize user input\n",
    "user_vector = preprocess_user_input(user_input, vectorizer)\n",
    "\n",
    "# Predict medication\n",
    "user_prediction = model.predict(user_vector)\n",
    "predicted_medicine_index = user_prediction.argmax(axis=1)\n",
    "recommended_medicine = label_encoder.inverse_transform(predicted_medicine_index)\n",
    "\n",
    "print(f\"Recommended Medicine: {recommended_medicine[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
